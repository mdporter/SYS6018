---
title: "Homework #2: Resampling" 
author: "**Your Name Here**"
date: "Due: Wed Feb 15 | 11:45am"
output: R6018::homework
---

**SYS 6018 | Sp 2023 | University of Virginia**

------------------------------------------------------------------------

```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6018")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```


# Required R packages and Directories


::: {.solution}
```{r packages, message=FALSE, warning=FALSE}
data.dir = 'https://mdporter.github.io/SYS6018/data/' # data directory
library(R6018)     # functions for SYS-6018
library(tidyverse) # functions for data manipulation 
library(tidymodels)# optional functions for tidymodels framework
```
:::



# Problem 1: Bootstrapping 

Bootstrap resampling can be used to quantify the uncertainty in a fitted curve. 

## a. Generate Data

Create a set of functions to generate data from the following distributions:
\begin{align*}
X &\sim \mathcal{U}(0, 2) \qquad \text{Uniform in $[0,2]$}\\
Y &= 1 + 2x + 5\sin(5x) + \epsilon \\
\epsilon &\sim \mathcal{N}(0,\, \sigma=2.5)
\end{align*}

::: {.solution}
Add solution here
:::

## b. Simulate $n=100$ realizations from these distributions. 

- Produce a scatterplot and draw the true regression line $f(x) = E[Y \mid X=x]$. 
- Use `set.seed(211)` prior to generating the data.

::: {.solution}
Add solution here
:::


## c. Fit a 5th degree polynomial. 

- Produce a scatterplot and draw the *estimated* regression curve.


::: {.solution}
Add solution here
:::

## d. Bootstrap sampling

Make 200 bootstrap samples. For each bootstrap sample, fit a 5th degree polynomial and make predictions at `eval_pts = seq(0, 2, length=100)`

- Set the seed (use `set.seed(212)`) so your results are reproducible.
- Produce a scatterplot with the original data and add the 200 bootstrap curves

::: {.solution}
Add solution here
:::

    
## e. Calculate the pointwise 95% confidence intervals from the bootstrap samples. 

That is, for each $x \in {\rm eval\_pts}$, calculate the upper and lower limits such that only 5% of the curves fall outside the interval at $x$. 

- Remake the plot from part *c*, but add the upper and lower boundaries from the 95% confidence intervals. 

::: {.solution}
Add solution here
:::


# Problem 2: V-Fold cross-validation with $k$ nearest neighbors

Run 10-fold cross-validation on the data generated in part 1b to select the optimal $k$ in a k-nearest neighbor (kNN) model. Then evaluate how well cross-validation performed by evaluating the performance on a large test set. The steps below will guide you.


## a. Select $k$ using $10$-fold cross-validation

Use $10$-fold cross-validation to find the value of $k$ (i.e., neighborhood size) that provides the smallest cross-validated MSE using a kNN model. 

- Search over $k=3,4,\ldots, 40$.
- Use `set.seed(221)` prior to generating the folds to ensure the results are replicable. 
- Show the following:
    - the optimal $k$ (as determined by cross-validation)
    - the corresponding estimated MSE
    - produce a plot with $k$ on the x-axis and the estimated MSE on the y-axis (optional: add 1-standard error bars). 
- Notation: The $k$ is the tuning paramter for the kNN model. The $v=10$ is the number of folds in V-fold cross-validation. Don't get yourself confused.

::: {.solution}
Add solution here
:::



## b. What is the optimal *edf* for the kNN model?

The $k$ (number of neighbors) in a kNN model determines the effective degrees of freedom *edf*. Be sure to use the correct sample size when making this calculation. 

- Produce a plot similar to that from part *a*, but use *edf* (effective degrees of freedom) on the x-axis. 

::: {.solution}
Add solution here
:::



## c. Optimal $k$ for final model

After running cross-validation, a final model fit from *all* of the training data needs to be produced to make predictions. What value of $k$ would you choose? Why? 


::: {.solution}
Add solution here
:::

## d. Cross-validation performance

Simulate a test data set of $50000$ observations from the same distributions. Use `set.seed(223)` prior to generating the test data. 

- Fit a set of kNN models, using the full training data, and calculate the mean squared error (MSE) on the test data for each model. Use the same $k$ values in *a*. 
- Report the optimal $k$, the corresponding *edf*, and MSE based on the **test set**.

::: {.solution}
Add solution here
:::

## e. Performance Plots

Plot both the cross-validation estimated and (true) error calculated from the test data on the same plot. See Figure 5.6 in ISL (pg 182) as a guide. 

- Produce two plots: one with $k$ on the x-axis and one with *edf* on the x-axis.
- Each plot should have two lines: one from part *a* and one from part *d* 
    
::: {.solution}
Add solution here
:::
    
## f. Did cross-validation work?

Based on the plots from *e*, does it appear that cross-validation worked as intended? How sensitive is the choice of $k$ on the resulting test MSE?      

::: {.solution}
Add solution here
:::








