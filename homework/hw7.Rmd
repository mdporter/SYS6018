---
title: "Homework #7: Tree Ensembles" 
author: "**Your Name Here**"
date: "Due: Wed Apr 05 | 11:45am"
output: R6018::homework
---

**SYS 4582/6018 | Spring 2023 | University of Virginia **

*******************************************
```{r config, echo=FALSE}
source(system.file("config/hw_config.R", package="R6018")) # knitr settings
# options(dplyr.summarise.inform = FALSE)  # ignore dplyr message about grouping
```


::: {style="background-color:yellow; color:red; display: block; border-color: black; padding:1em"}

This is an **independent assignment**. Do not discuss or work with classmates.

:::


# Required R packages and Directories {-}

::: {.solution}
```{r packages, message=FALSE, warning=FALSE}
data_dir = 'https://mdporter.github.io/SYS6018/data/' # data directory
library(R6018)     # functions for SYS-6018
library(tidyverse) # functions for data manipulation  
library(randomForest)
# Add other libraries here
```
:::

# Problem 1: Tree Splitting for classification

Consider the Gini index, classification error, and entropy impurity measures in a simple classification setting with two classes. 

Create a single plot that displays each of these quantities as a function of $p_m$, the estimated probability of an observation in node $m$ being from class 1. The x-axis should display $p_m$, ranging from 0 to 1, and the y-axis should display the value of the Gini index, classification error, and entropy.

::: {.solution}

Add solution here

:::


# Problem 2: Combining bootstrap estimates

```{r, echo=FALSE}
p_red = c(0.2, 0.25, 0.3, 0.4, 0.4, 0.45, 0.7, 0.85, 0.9, 0.9)
```

Suppose we produce ten bootstrapped samples from a data set containing red and green classes. We then apply a classification tree to each bootstrapped sample and, for a specific value of $X$, produce the following 10 estimates of $\Pr(\text{Class is Red} \mid X)$: $\{`r stringr::str_c(p_red, sep=", ")`\}$.

## a. Majority Voting

ISLR 8.2 describes the *majority vote* approach for making a hard classification from a set of bagged classifiers. What is the final classification for this example using majority voting?

::: {.solution}

Add solution here

:::


## b. Average Probability

An alternative is to base the final classification on the average probability. What is the final classification for this example using average probability?

::: {.solution}

Add solution here

:::



## c. Unequal Cost of Misclassification

Suppose the cost of mis-classifying a Red observation (as Green) is twice as costly as mis-classifying a Green observation (as Red). How would you modify both approaches to make better final classifications under these unequal costs? Report the final classifications. 

::: {.solution}

Add solution here

:::



# Problem 3: Random Forest Tuning

Random forest has several tuning parameters that you will explore in this problem. We will use the `Boston` housing data from the `MASS` R package (See the ISLR Lab in section 8.3.3 for example code).

- Note: remember that `MASS` can mask the `dplyr::select()` function.

## a. Tuning Parameters

List all of the random forest tuning parameters in the `randomForest::randomForest()` function. Note any tuning parameters that are specific to classification or regression problems. Indicate the tuning parameters you think will be most important to optimize? 

::: {.solution}

Add solution here

:::



## b. RF CV performance

Use a random forest model to predict `medv`, the median value of owner-occupied homes (in $1000s). Use the default parameters and report the 10-fold cross-validation MSE. 

::: {.solution}

Add solution here

:::



## c. Effects of `mtry` and `ntree`

Now we will vary the tuning parameters of `mtry` and `ntree` to see what effect they have on performance. 

- Use a range of reasonable `mtry` and `ntree` values.
- Use 5 times repeated out-of-bag (OOB) to assess performance. That is, run random forest 5 times for each tuning set, calculate the OOB MSE each time and use the average for the MSE associated with the tuning parameters.
- Use a plot to show the average MSE as a function of `mtry` and `ntree`.
- Report the best tuning parameter combination. 
- Note: random forest is a stochastic model; it will be different every time it runs. Set the random seed to control the uncertainty associated with the stochasticity. 
- Hint: If you use the `randomForest` package, the `mse` element in the output is a vector of OOB MSE values for `1:ntree` trees in the forest. This means that you can set `ntree` to some maximum value and get the MSE for any number of trees up to `ntree`. 


::: {.solution}

Add solution here

:::
